{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SMKSmith/deep-learning-challenge/blob/main/AlphabetSoupCharity_Optimisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing**"
      ],
      "metadata": {
        "id": "1q1nhZRbclX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import our dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "#  Import and read the charity_data.csv.\n",
        "import pandas as pd \n",
        "application_df = pd.read_csv(\"https://static.bc-edx.com/data/dla-1-2/m21/lms/starter/charity_data.csv\")\n",
        "application_df.head()"
      ],
      "metadata": {
        "id": "STYWKhsZcyHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns, 'EIN' and 'NAME'.\n",
        "app_df = application_df.drop(columns= ['EIN'])\n",
        "app_df"
      ],
      "metadata": {
        "id": "14ex9hXzc6c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the number of unique values in each column.\n",
        "app_df.nunique()"
      ],
      "metadata": {
        "id": "LND5ub-tdIKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use value counts to determine balancing\n",
        "app_df[\"APPLICATION_TYPE\"].value_counts()"
      ],
      "metadata": {
        "id": "1tDdgFIwdQq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a cutoff value and create a list of application types to be replaced\n",
        "# use the variable name `application_types_to_replace`\n",
        "app_to_replace = ['T9','T13','T12','T2','T25', 'T14', 'T29', 'T15', 'T17']\n",
        "\n",
        "# Replace in dataframe\n",
        "for app in app_to_replace:\n",
        "    app_df['APPLICATION_TYPE'] = app_df['APPLICATION_TYPE'].replace(app,\"Other_1\")\n",
        "\n",
        "# Check binning was successful\n",
        "app_df['APPLICATION_TYPE'].value_counts()"
      ],
      "metadata": {
        "id": "wII9fKvrdhoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at CLASSIFICATION value counts for binning\n",
        "app_count = app_df[\"CLASSIFICATION\"].value_counts()\n",
        "\n",
        "app_count[app_count>9]"
      ],
      "metadata": {
        "id": "hLEKWUBqd287"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a cutoff value and create a list of classifications to be replaced\n",
        "# use the variable name `classifications_to_replace`\n",
        "\n",
        "class_replace = app_count[app_count<100].index\n",
        "\n",
        "# Replace in dataframe\n",
        "for cls in class_replace:\n",
        "    app_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other_2\")\n",
        "    \n",
        "# Check to make sure binning was successful\n",
        "app_df['CLASSIFICATION'].value_counts()"
      ],
      "metadata": {
        "id": "ntEVWbmNecrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show binning values for NAME\n",
        "app_df[\"NAME\"].value_counts()"
      ],
      "metadata": {
        "id": "pA3S-76Xe2Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show binning values for CLASSIFICATION \n",
        "app_df_NAME = app_df[\"NAME\"].value_counts()\n",
        "\n",
        "app_df_NAME[app_df_NAME > 400]"
      ],
      "metadata": {
        "id": "lkD3fvNUe-2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a cutoff value and create a list of classifications to be replaced\n",
        "# use the variable name `classifications_to_replace`\n",
        "NAME_replace = app_df_NAME[app_df_NAME < 100].index\n",
        "\n",
        "# Replace in dataframe\n",
        "for othername in NAME_replace:\n",
        "    app_df['NAME'] = app_df['NAME'].replace(othername,\"Other_3\")\n",
        "    \n",
        "# Show binning values\n",
        "app_df['NAME'].value_counts()"
      ],
      "metadata": {
        "id": "Vp-pNhwPfKdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to numeric with `pd.get_dummies`\n",
        "dum_NAME = pd.get_dummies(app_df['NAME'])\n",
        "dum_APPLICATION_TYPE = pd.get_dummies(app_df['APPLICATION_TYPE'])\n",
        "dum_CLASSIFICATION = pd.get_dummies(app_df['CLASSIFICATION'])\n",
        "dum_AFFILIATION = pd.get_dummies(app_df['AFFILIATION'])\n",
        "dum_USE_CASE = pd.get_dummies(app_df['USE_CASE'])\n",
        "dum_ORGANIZATION = pd.get_dummies(app_df['ORGANIZATION'])\n",
        "dum_INCOME_AMT = pd.get_dummies(app_df['INCOME_AMT'])\n",
        "dum_SPECIAL_CONSIDERATIONS = pd.get_dummies(app_df['SPECIAL_CONSIDERATIONS'])\n",
        "\n",
        "concat1 = pd.concat([app_df, dummies_NAME, dummies_APPLICATION_TYPE, dummies_CLASSIFICATION, dummies_AFFILIATION, dummies_USE_CASE, dummies_ORGANIZATION,dummies_INCOME_AMT], axis=\"columns\")\n",
        "concat1"
      ],
      "metadata": {
        "id": "VuPRo2U9gJoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop categorical data\n",
        "concat_df = concat1.drop(columns=['NAME','APPLICATION_TYPE', 'CLASSIFICATION','AFFILIATION', 'USE_CASE', 'ORGANIZATION', 'INCOME_AMT',\"SPECIAL_CONSIDERATIONS\"])\n",
        "concat_df"
      ],
      "metadata": {
        "id": "6Q6XXOA4g2Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our preprocessed data into our features and target arrays\n",
        "y = concat_df['IS_SUCCESSFUL']\n",
        "X = concat_df.drop(columns='IS_SUCCESSFUL')\n",
        "\n",
        "# Split the preprocessed data into a training and testing dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "P5qDzofwhBGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "vL9PaMSkhK62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "8MRsVErYhOOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a StandardScaler instances\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "sNf1ireMhQp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Compile, Train and Evaluate the Model**"
      ],
      "metadata": {
        "id": "0SlmVxDpiDAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
        "\n",
        "nn = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=20, activation=\"relu\", input_dim=137))\n",
        "\n",
        "# Second hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=30, activation=\"tanh\"))\n",
        "\n",
        "# third hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=20, activation=\"sigmoid\"))\n",
        "\n",
        "# fouth hidder layer\n",
        "nn.add(tf.keras.layers.Dense(units=10, activation=\"relu\"))\n",
        "\n",
        "# Output layer\n",
        "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Check the structure of the model\n",
        "nn.summary()"
      ],
      "metadata": {
        "id": "7yMm6XQ3iL12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "usLTmbd6iXEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model_fit = nn.fit(X_train_scaled, y_train, epochs=10)"
      ],
      "metadata": {
        "id": "II8N7xDWifgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "metadata": {
        "id": "M385nnWNiq0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export our model to HDF5 file\n",
        "nn.save('AlphabetSoupCharity_Optimisation.ipynb')"
      ],
      "metadata": {
        "id": "lZ6NDOS9iw3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Alternative Model: the Decision Tree Model**"
      ],
      "metadata": {
        "id": "oq9i8zMGi3sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from sklearn import tree\n",
        " # Creating the decision tree classifier instance\n",
        "model = tree.DecisionTreeClassifier()"
      ],
      "metadata": {
        "id": "cFCwjEZ9jAND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Fit the model\n",
        "model = model.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "LJa-ub6DjEps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Make predictions using testing data\n",
        "predictions = model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "hdhPAubLjK7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "cmatrix = confusion_matrix(y_test, predictions)\n",
        "cmatrix_df = pd.DataFrame(\n",
        "    cmatrix, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]\n",
        ")\n",
        "\n",
        "# Calculating the accuracy score\n",
        "accu_score = accuracy_score(y_test, predictions)"
      ],
      "metadata": {
        "id": "uF2JGNFLjSCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying results\n",
        "print(\"Confusion Matrix\")\n",
        "display(cmatrix_df)\n",
        "print(f\"Accuracy Score : {accu_score}\")\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "id": "FmoNzVO-jleb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Alternative model: the Random Forest Model**"
      ],
      "metadata": {
        "id": "Q7JcLHYkjw1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Create a random forest classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rforest_model = RandomForestClassifier(n_estimators=500, random_state=78)"
      ],
      "metadata": {
        "id": "K4uaYdpFkBoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Fitting the model\n",
        "rforest_model = rforest_model.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "EI681cBfkL3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Make predictions using the testing data\n",
        "predictions = rforest_model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "naMi8D19kPrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the confusion matrix\n",
        "cmatrix = confusion_matrix(y_test, predictions)\n",
        "cmatrix_df = pd.DataFrame(\n",
        "    cmatrix, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]\n",
        ")\n",
        "\n",
        "# Calculate the accuracy score\n",
        "accu_score = accuracy_score(y_test, predictions)"
      ],
      "metadata": {
        "id": "De2-wRKVkSGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Displaying results\n",
        "print(\"Confusion Matrix\")\n",
        "display(cmatrix_df)\n",
        "print(f\"Accuracy Score : {accu_score}\")\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "id": "EsHlEBKDkpNT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}